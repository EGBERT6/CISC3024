# Import libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torch.utils.data import DataLoader
from tqdm import tqdm
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd

# Check device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define the SmallVGG model
class SmallVGG(nn.Module):
    def __init__(self):
        super(SmallVGG, self).__init__()
        self.conv_layers = nn.Sequential(
            nn.Conv2d(3, 8, kernel_size=3, padding=1), nn.ReLU(),
            nn.Conv2d(8, 16, kernel_size=3, padding=1), nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.ReLU(),
            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(),
            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )

        self.fc_layers = nn.Sequential(
            nn.Linear(32 * 3 * 3, 256), nn.ReLU(),
            nn.Linear(256, 10)
        )

    def forward(self, x):
        x = self.conv_layers(x)
        x = x.view(x.size(0), -1)
        x = self.fc_layers(x)
        return x

# Data transformations
transform = transforms.Compose([
    transforms.Resize((28, 28)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Load datasets
train_dataset = datasets.SVHN(root='./data', split='train', download=True, transform=transform)
test_dataset = datasets.SVHN(root='./data', split='test', download=True, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)

# Initialize model, criterion, and optimizer
model = SmallVGG().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training and evaluation function
def train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, num_epochs=30):
    train_losses = []
    test_losses = []
    all_labels = []
    all_preds = []
    all_probs = []

    for epoch in range(num_epochs):
        # Training phase
        model.train()
        running_loss = 0.0
        for images, labels in tqdm(train_loader, desc=f'Training Epoch {epoch + 1}'):
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * len(images)

        train_losses.append(running_loss / len(train_loader.dataset))

        # Evaluation phase
        model.eval()
        test_loss = 0.0
        with torch.no_grad():
            for images, labels in test_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                test_loss += loss.item() * len(images)

                # Get predictions and probabilities
                preds = torch.argmax(outputs, dim=1)
                probs = torch.softmax(outputs, dim=1)

                # Accumulate test labels and predictions
                all_labels.extend(labels.cpu().numpy())
                all_preds.extend(preds.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())

        test_losses.append(test_loss / len(test_loader.dataset))

        # Calculate evaluation metrics
        accuracy = accuracy_score(all_labels, all_preds)
        macro_auc = roc_auc_score(all_labels, all_probs, average="macro", multi_class="ovr")
        micro_auc = roc_auc_score(all_labels, all_probs, average="micro", multi_class="ovr")
        
        # Print current epoch loss and metrics
        print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}")
        print(f"Accuracy: {accuracy:.2f}, Macro AUC: {macro_auc:.2f}, Micro AUC: {micro_auc:.2f}")

    return train_losses, test_losses, accuracy, macro_auc, micro_auc, all_labels, all_preds, all_probs

# Execute training and evaluation
num_epochs = 30
train_losses, test_losses, accuracy, macro_auc, micro_auc, all_labels, all_preds, all_probs = train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, num_epochs)

# Plot training and testing loss curves
plt.figure(figsize=(6, 6))
plt.plot(train_losses, label="Training Loss")
plt.plot(test_losses, label="Testing Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training and Testing Loss Curves")
plt.legend()
plt.show()

# Calculate ROC curves for each class
plt.figure(figsize=(8, 8))
for i in range(len(all_probs[0])):
    fpr, tpr, _ = roc_curve([1 if label == i else 0 for label in all_labels], [prob[i] for prob in all_probs])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--', label="Random Guess")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves for Each Class")
plt.legend()
plt.show()

# Calculate class-wise accuracy
num_classes = 10
class_accuracies = [(np.sum((all_preds == i) & (all_labels == i)) / np.sum(all_labels == i) if np.sum(all_labels == i) > 0 else 0) for i in range(num_classes)]

# Plot class-wise accuracy
plt.figure(figsize=(10, 5))
sns.barplot(x=list(range(num_classes)), y=class_accuracies)
plt.xlabel("Class")
plt.ylabel("Accuracy")
plt.title("Class-wise Accuracy")
plt.show()

# Calculate confusion matrix and plot
conf_matrix = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=list(range(num_classes)), yticklabels=list(range(num_classes)))
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

# Calculate ROC curves and AUCs for each class
fpr = {}
tpr = {}
roc_auc = {}

for i in range(num_classes):
    fpr[i], tpr[i], _ = roc_curve(np.array(all_labels) == i, np.array(all_probs)[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Prepare metrics summary table
metrics = [
    {"Metric": "Overall Accuracy", "Value": accuracy},
    {"Metric": "Macro AUC", "Value": macro_auc},
    {"Metric": "Micro AUC", "Value": micro_auc}
]

# Add per-class AUC values
for i in range(num_classes):
    metrics.append({"Metric": f"AUC for Class {i}", "Value": roc_auc[i]})

# Create DataFrame for summary table
summary_table = pd.DataFrame(metrics)
print(summary_table)


